{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using screenshots to visualise change in a page over time\n",
    "\n",
    "![Screenshots showing changes to the ABC Australia home page over time](images/abc-net-au.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"alert alert-warning\">Work in progress â€“ this notebook isn't finished yet. Check back later for more...<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import PIL\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import io\n",
    "import base64\n",
    "import time\n",
    "import re\n",
    "from slugify import slugify\n",
    "from webdriverdownloader import GeckoDriverDownloader\n",
    "from pathlib import Path\n",
    "\n",
    "gdd = GeckoDriverDownloader()\n",
    "geckodriver = gdd.download_and_install(\"v0.26.0\")[1]\n",
    "\n",
    "# See https://github.com/ouseful-template-repos/binder-selenium-demoscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_cdx(url, **kwargs):\n",
    "    '''\n",
    "    Query the IA CDX API for the supplied url.\n",
    "    You can optionally provide any of the parameters accepted by the API.\n",
    "    '''\n",
    "    params = {\n",
    "        'url': url,\n",
    "        'output': 'json',\n",
    "        'filter': ['statuscode:200', 'mimetype:text/html']\n",
    "    }\n",
    "    # User-Agent value is necessary or else IA gives an error\n",
    "    response = requests.get('http://web.archive.org/cdx/search/cdx', params=params, headers={'User-Agent': ''})\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def get_full_page_screenshot(url, save_width=200):\n",
    "    '''\n",
    "    Gets a full page screenshot of the supplied url.\n",
    "    By default resizes the screenshot to a maximum width of 200px.\n",
    "    Provide a 'save_width' value to change this.\n",
    "    '''\n",
    "    print(url)\n",
    "    date_str, site = re.search(r'\\/web\\/(\\d+)if_\\/https*:\\/\\/(.+\\/)', url).groups()\n",
    "    ss_file = Path('screenshots', slugify(site), f'{slugify(site)}-{date_str}.png')\n",
    "    if not ss_file.exists():\n",
    "        options = webdriver.FirefoxOptions()\n",
    "        options.headless = True\n",
    "        driver = webdriver.Firefox(executable_path=geckodriver, options=options)\n",
    "        driver.get(url)\n",
    "        # Give some time for everything to load\n",
    "        time.sleep(5)\n",
    "        # Can just use maximize_window() instead of the below (only in Geckodriver?)\n",
    "        # S = lambda X: driver.execute_script('return document.body.parentNode.scroll'+X)\n",
    "        # driver.set_window_size(capture_width, S('Height') + 50) # May need manual adjustment\n",
    "        driver.maximize_window()\n",
    "        current_width = driver.get_window_size()['width']\n",
    "        try:\n",
    "            ss = driver.find_element_by_tag_name('body').screenshot_as_base64\n",
    "        except NoSuchElementException:\n",
    "            ss = driver.find_element_by_tag_name('frameset').screenshot_as_base64\n",
    "        driver.quit()\n",
    "        img = Image.open(io.BytesIO(base64.b64decode(ss)))\n",
    "        ratio = save_width / current_width\n",
    "        (width, height) = (round(img.width * ratio), round(img.height * ratio))\n",
    "        resized_img = img.resize((width, height), PIL.Image.LANCZOS)\n",
    "        resized_img.save(ss_file)\n",
    "        \n",
    "def get_screenshots(domain, num=1):\n",
    "    '''\n",
    "    Generate up to the specified number of screenshots for each year.\n",
    "    Queries CDX API for snapshots of the given url,\n",
    "    then gets the first 'num' timestamps for each year.\n",
    "    '''\n",
    "    #This only gets the first pages of results -- change to page through, or could I collapse the CDX results?\n",
    "    data = query_cdx(domain, num=1)\n",
    "    df = pd.DataFrame(data[1:], columns=data[0])\n",
    "    # Convert the timestamp string into a datetime object\n",
    "    df['date'] = pd.to_datetime(df['timestamp'])\n",
    "    # Sort by date\n",
    "    df.sort_values(by=['date'], inplace=True)\n",
    "    # Only keep the first instance of each digest\n",
    "    df.drop_duplicates(subset=['digest'], inplace=True)\n",
    "    # Extract year from date\n",
    "    df['year'] = df['date'].dt.year\n",
    "    # Get the first 'num' instances from each year \n",
    "    # (you only need one, but if there are failures, you might want a backup)\n",
    "    df_years = df.groupby('year', as_index=False).head(num)\n",
    "    timestamps = df_years['timestamp'].to_list()\n",
    "    Path('screenshots', slugify(domain)).mkdir(parents=True, exist_ok=True)\n",
    "    # if_ gives you pages without the IA nav, but with the CSS links etc rewritten!\n",
    "    for timestamp in timestamps:\n",
    "        url = f'https://web.archive.org/web/{timestamp}if_/http://{domain}/'\n",
    "        get_full_page_screenshot(url)\n",
    "        \n",
    "def make_composite(domain):\n",
    "    '''\n",
    "    Combine single screenshots into a composite image.\n",
    "    Loops through images in a directory with the given (slugified) domain.\n",
    "    '''\n",
    "    max_height = 0\n",
    "    pngs = sorted(Path('screenshots', slugify(domain)).glob('*.png'))\n",
    "    for png in pngs:\n",
    "        img = Image.open(png)\n",
    "        if img.height > max_height:\n",
    "            max_height = img.height\n",
    "    comp = Image.new('RGB', ((len(pngs) * 200) + ((len(pngs) - 1) * 10), max_height + 50), (90,90,90))\n",
    "    # Canvas to write in the dates\n",
    "    draw = ImageDraw.Draw(comp)\n",
    "    # Change this to suit your system\n",
    "    font = ImageFont.truetype(\"/Library/Fonts/Microsoft/Gill Sans MT Bold.ttf\", 36)\n",
    "    for i, png in enumerate(pngs):\n",
    "        year = re.search(r'-(\\d{4})\\d+.png', png.name).group(1)\n",
    "        draw.text((i * 210, 10), year,(255,255,255),font=font)\n",
    "        img = Image.open(png)\n",
    "        comp.paste(img, (i * 210, 50))\n",
    "    comp.save(Path('screenshots', f'{slugify(domain)}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://web.archive.org/web/19961017233008if_/http://abc.net.au/\n",
      "https://web.archive.org/web/19970103063844if_/http://abc.net.au/\n",
      "https://web.archive.org/web/19980131014243if_/http://abc.net.au/\n",
      "https://web.archive.org/web/19990116225807if_/http://abc.net.au/\n",
      "https://web.archive.org/web/20000229082857if_/http://abc.net.au/\n",
      "https://web.archive.org/web/20010305163334if_/http://abc.net.au/\n",
      "https://web.archive.org/web/20020122073806if_/http://abc.net.au/\n",
      "https://web.archive.org/web/20030129150102if_/http://abc.net.au/\n",
      "https://web.archive.org/web/20040101013727if_/http://abc.net.au/\n",
      "https://web.archive.org/web/20050101010752if_/http://abc.net.au/\n",
      "https://web.archive.org/web/20060101013429if_/http://abc.net.au/\n",
      "https://web.archive.org/web/20070105191459if_/http://abc.net.au/\n",
      "https://web.archive.org/web/20080101014408if_/http://abc.net.au/\n",
      "https://web.archive.org/web/20090103033228if_/http://abc.net.au/\n",
      "https://web.archive.org/web/20100107000734if_/http://abc.net.au/\n",
      "https://web.archive.org/web/20110103005814if_/http://abc.net.au/\n",
      "https://web.archive.org/web/20120101215728if_/http://abc.net.au/\n",
      "https://web.archive.org/web/20130101045837if_/http://abc.net.au/\n",
      "https://web.archive.org/web/20140101001924if_/http://abc.net.au/\n",
      "https://web.archive.org/web/20150101025757if_/http://abc.net.au/\n",
      "https://web.archive.org/web/20160101002724if_/http://abc.net.au/\n",
      "https://web.archive.org/web/20170101010931if_/http://abc.net.au/\n",
      "https://web.archive.org/web/20180101042556if_/http://abc.net.au/\n",
      "https://web.archive.org/web/20190101012614if_/http://abc.net.au/\n",
      "https://web.archive.org/web/20200101015356if_/http://abc.net.au/\n"
     ]
    }
   ],
   "source": [
    "get_screenshots('abc.net.au')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_composite('abc.net.au')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Work on this notebook was supported by the [IIPC Discretionary Funding Programme 2019-2020](http://netpreserve.org/projects/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
