# Possible notebooks

## CDX data

* tracking an individual site over time
* comparing sites over time
* harvesting subdomains


## Other

* Comparing versions of a page
* sampling sites?

## Resources

* https://github.com/DocNow/waybackprov

Sources of data are:
    
* CDX APIs â€“ index of captures, allow complex queries (such as all snapshots from a particulr domain)
* [Memento APIs](https://timetravel.mementoweb.org/guide/api/)

Info:

* [CDX File Format](http://iipc.github.io/warc-specifications/specifications/cdx-format/cdx-2015/) (from IIPC)
* [Archive-it CDX/C API](https://support.archive-it.org/hc/en-us/articles/115001790023-Access-Archive-It-s-Wayback-index-with-the-CDX-C-API)
* [IA Wayback APIs](https://archive.org/help/wayback_api.php)
* [pywb CDX API documentation](https://github.com/webrecorder/pywb/wiki/CDX-Server-API)
* [Common Crawl indexes](http://index.commoncrawl.org/) & [all data](https://commoncrawl.org/the-data/get-started/)
* [Awesome Web Archiving](http://netpreserve.org/web-archiving/tools-and-software/)
* [Stanford Uni resaecrh resources](https://library.stanford.edu/projects/web-archiving/research-resources/data-formats-and-apis)
* [Hashes](https://blogs.loc.gov/thesignal/2011/11/hashing-out-digital-trust/?loclr=blogsig)

Tools:

* [CDX Toolkit](https://pypi.org/project/cdx-toolkit/0.9.2/)
* 

What publicly available CDX APIs are there?

* IA 
* Archive-It (how much is public?)
* Common Crawl (separate index for each harvest)

Other CDX data:

* [UKWA](https://data.webarchive.org.uk/opendata/ukwa.ds.2/cdx/)