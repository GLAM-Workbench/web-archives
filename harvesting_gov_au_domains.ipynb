{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvesting Australian government domains from the CDX index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"alert alert-warning\">Work in progress â€“ this notebook isn't finished yet. Check back later for more...<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that collapse doesn't seem to work with domain matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import requests_cache\n",
    "import time\n",
    "from requests_cache import CachedSession\n",
    "from tinydb import TinyDB\n",
    "\n",
    "s = CachedSession()\n",
    "retries = Retry(total=10, backoff_factor=1, status_forcelist=[ 502, 503, 504 ])\n",
    "s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "s.mount('http://', HTTPAdapter(max_retries=retries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_resumption_key(results):\n",
    "    '''\n",
    "    Checks to see if the second-last row is an empty list,\n",
    "    if it is, return the last value as the resumption key.\n",
    "    '''\n",
    "    try:\n",
    "        if not results[-2]:\n",
    "            return results[-1][0]\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "def get_total_pages(params):\n",
    "    '''\n",
    "    Gets the total number of pages in a set of results.\n",
    "    '''\n",
    "    these_params = params.copy()\n",
    "    these_params['showNumPages'] = 'true'\n",
    "    response = s.get('http://web.archive.org/cdx/search/cdx', params=these_params, headers={'User-Agent': ''})\n",
    "    return int(response.text)\n",
    "\n",
    "def prepare_params(url, use_resume_key=False, **kwargs):\n",
    "    '''\n",
    "    Prepare the parameters for a CDX API requests.\n",
    "    Adds all supplied keyword arguments as parameters (changing from_ to from).\n",
    "    Adds in a few necessary parameters and showResumeKey if requested.\n",
    "    '''\n",
    "    params = kwargs\n",
    "    params['url'] = url\n",
    "    params['output'] = 'json'\n",
    "    if use_resume_key:\n",
    "        params['showResumeKey'] = 'true'\n",
    "    # CDX accepts a 'from' parameter, but this is a reserved word in Python\n",
    "    # Use 'from_' to pass the value to the function & here we'll change it back to 'from'.\n",
    "    if 'from_' in params:\n",
    "        params['from'] = params['from_']\n",
    "        del(params['from_'])\n",
    "    return params\n",
    "\n",
    "def get_cdx_data(params):\n",
    "    '''\n",
    "    Make a request to the CDX API using the supplied parameters.\n",
    "    Check the results for a resumption key, and return the key (if any) and the results.\n",
    "    '''\n",
    "    response = s.get('http://web.archive.org/cdx/search/cdx', params=params, headers={'User-Agent': ''})\n",
    "    response.raise_for_status()\n",
    "    results = response.json()\n",
    "    resumption_key = check_for_resumption_key(results)\n",
    "    # Remove the resumption key from the results\n",
    "    if resumption_key:\n",
    "        results = results[:-2]\n",
    "    if not response.from_cache:\n",
    "        time.sleep(0.2)\n",
    "    return resumption_key, results\n",
    "\n",
    "def convert_lists_to_dicts(results):\n",
    "    if results:\n",
    "        keys = results[0]\n",
    "        results_as_dicts = [dict(zip(keys, v)) for v in results[1:]]\n",
    "    else:\n",
    "        results_as_dicts = results\n",
    "    return results_as_dicts\n",
    "\n",
    "def query_cdx_by_page(url, **kwargs):\n",
    "    db = TinyDB('db.json')\n",
    "    db.purge()\n",
    "    page = 0\n",
    "    params = prepare_params(url, **kwargs)\n",
    "    total_pages = get_total_pages(params)\n",
    "    with tqdm(total=total_pages-page) as pbar1:\n",
    "        with tqdm() as pbar2:\n",
    "            while page < total_pages:\n",
    "                params['page'] = page\n",
    "                _, results = get_cdx_data(params)\n",
    "                db.insert_multiple(convert_lists_to_dicts(results))\n",
    "                page += 1\n",
    "                pbar1.update(1)\n",
    "                pbar2.update(len(results) - 1)\n",
    "\n",
    "def query_cdx_with_key(url, **kwargs):\n",
    "    '''\n",
    "    Harvest results from the CDX API using the supplied parameters.\n",
    "    Uses showResumeKey to check if there are more than one page of results,\n",
    "    and if so loops through pages until all results are downloaded.\n",
    "    '''\n",
    "    params = prepare_params(url, use_resume_key=True, **kwargs)\n",
    "    with tqdm() as pbar:\n",
    "        # This will include the header row\n",
    "        resumption_key, all_results = get_cdx_data(params)\n",
    "        pbar.update(len(all_results) - 1)\n",
    "        while resumption_key is not None:\n",
    "            params['resumeKey'] = resumption_key\n",
    "            resumption_key, results = get_cdx_data(params)\n",
    "            # Remove the header row and add\n",
    "            all_results += results[1:]\n",
    "            pbar.update(len(results) - 1)\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an overview of the domain -- unique urlkeys only\n",
    "# Then get all the 3 level domains and harvest separately collapsed on year to give chnage over time\n",
    "# Trying to balance, speed, performance, file size, memory etc\n",
    "# Note than harvesting a domain has the same number of pages (ie requests) no matter what filters are applied -- it's just that some pages will be empty.\n",
    "# So repeating a domain harvest with different filters will mean less data, but the same number of requests.\n",
    "# What's most efficient? I dunno.\n",
    "results = query_cdx_by_page('*.gov.au', filter=['statuscode:200', 'mimetype:text/html'], collapse='urlkey', fl='urlkey', pageSize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = TinyDB('db.json')\n",
    "df = pd.DataFrame(db.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urlkey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>au,gov,naa)/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>au,gov,naa)/../the_collection/cabinet/1967_cab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>au,gov,naa)/?c=a.checked:b===</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>au,gov,naa)/?sssdmh=dm13.167154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>au,gov,naa)/?sssdmh=dm13.167154/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              urlkey\n",
       "0                                       au,gov,naa)/\n",
       "1  au,gov,naa)/../the_collection/cabinet/1967_cab...\n",
       "2                      au,gov,naa)/?c=a.checked:b===\n",
       "3                    au,gov,naa)/?sssdmh=dm13.167154\n",
       "4                   au,gov,naa)/?sssdmh=dm13.167154/"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if d[-2]:\n",
    "    print('y')\n",
    "else:\n",
    "    print('n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results[1:], columns=results[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['urlkey'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_domains = df['urlkey'].str.split(')', expand=True)[0].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_domains.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_domains[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_domains.drop_duplicates(inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df_domains[0].str.split(',', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub[2].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub[4].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.loc[df_sub[4] == 'archcms01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvest_by_year()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov_domains = []\n",
    "total_pages = get_total_pages(params)\n",
    "page = 0\n",
    "with tqdm(total=total_pages-page) as pbar:\n",
    "    these_params = params.copy()\n",
    "    these_params['output'] = 'json'\n",
    "    while page < total_pages:\n",
    "        these_params['page'] = page\n",
    "        response = s.get('http://web.archive.org/cdx/search/cdx', params=these_params)\n",
    "        for capture in response.json()[1:]:\n",
    "            urlkey = capture[0]\n",
    "            domain = urlkey[:urlkey.find(')')]\n",
    "            if domain not in gov_domains:\n",
    "                gov_domains.append(domain)\n",
    "        page += 1\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = response.json()[0]\n",
    "df = pd.DataFrame(captures, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gov_domains.json', 'w') as json_file:\n",
    "    json.dump(gov_domains, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(domains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add 'id_' to timestamp of a web archive url to get the original html (ie not the replay version), eg: http://wayback.archive-it.org/all/20190630231630id_/http://discontents.com.au/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gov_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gov = pd.DataFrame(gov_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gov_split = df_gov[0].str.split(',', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gov_split.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gov_split[2].value_counts()[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gov_split.loc[df_gov_split[2] == 'dfat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gov_split.to_csv('gov_domains.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('http://web.archive.org/cdx/search/cdx?url=*.nsw.gov.au&filter=statuscode:200&filter=mimetype:text/html&collapse=urlkey&output=json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
