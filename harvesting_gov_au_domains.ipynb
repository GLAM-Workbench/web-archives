{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring subdomains in the whole of gov.au\n",
    "\n",
    "<p class=\"alert alert-info\">New to Jupyter notebooks? Try <a href=\"getting-started/Using_Jupyter_notebooks.ipynb\"><b>Using Jupyter notebooks</b></a> for a quick introduction.</p>\n",
    "\n",
    "Most of the notebooks in this repository work with small slices of web archive data. In this notebook we'll scale things up a bit to try and find all of the subdomains that have existed in the `gov.au` domain. As in other notebooks, we'll obtain the data by querying the Internet Archive's CDX API. The only real difference is that it will take some hours to harvest all the data.\n",
    "\n",
    "All we're interested in this time are unique domain names, so to minimise the amount of data we'll be harvesting we can make use of the CDX API's `collapse` parameter. By setting `collapse=urlkey` we can tell the CDX API to drop records with duplicate `urlkey` values – this should mean we only get one capture per page. However, this only works if the capture records are in adjacent rows, so there probably will still be some duplicates. We'll also use the `fl` to limit the fields returned, and the `filter` parameter to limit results by `statuscode` and `mimetype`. So the parameters we'll use are:\n",
    "\n",
    "* `url=*.gov.au` – all of the pages in all of the subdomains under `gov.au`\n",
    "* `collapse=urlkey` – as few captures per page as possible\n",
    "* `filter=statuscode:200,mimetype:text/html` – only successful captures of HTML pages\n",
    "* `fl=urlkey,timestamp,original` – only these fields\n",
    "\n",
    "Even with these limits, the query will retrieve a LOT of data. To make the harvesting process easier to manage and more robust, I'm going to make use of the `requests-cache` module. This will capture the results of all requests, so that if things get interrupted and we have to restart, we can retrieve already harvested requests from the cache without downloading them again. We'll also write the harvested results directly to disk rather than consuming all our computer's memory. The file format will be the NDJSON (Newline Delineated JSON) format – because each line is a separate JSON object we can just write it a line at a time as the data is received.\n",
    "\n",
    "For a general approach to harvesting domain-level information from the IA CDX API see [Harvesting data about a domain using the IA CDX API](harvesting_domain_data.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "from requests_cache import CachedSession\n",
    "import ndjson\n",
    "from pathlib import Path\n",
    "from slugify import slugify\n",
    "import arrow\n",
    "import json\n",
    "import re\n",
    "from newick import Node\n",
    "import newick\n",
    "from ete3 import Tree, TreeStyle\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, FileLink\n",
    "\n",
    "s = CachedSession()\n",
    "retries = Retry(total=10, backoff_factor=1, status_forcelist=[ 502, 503, 504 ])\n",
    "s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "s.mount('http://', HTTPAdapter(max_retries=retries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'gov.au'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_pages(params):\n",
    "    '''\n",
    "    Gets the total number of pages in a set of results.\n",
    "    '''\n",
    "    these_params = params.copy()\n",
    "    these_params['showNumPages'] = 'true'\n",
    "    response = s.get('http://web.archive.org/cdx/search/cdx', params=these_params, headers={'User-Agent': ''})\n",
    "    return int(response.text)\n",
    "\n",
    "def prepare_params(url, **kwargs):\n",
    "    '''\n",
    "    Prepare the parameters for a CDX API requests.\n",
    "    Adds all supplied keyword arguments as parameters (changing from_ to from).\n",
    "    Adds in a few necessary parameters.\n",
    "    '''\n",
    "    params = kwargs\n",
    "    params['url'] = url\n",
    "    params['output'] = 'json'\n",
    "    # CDX accepts a 'from' parameter, but this is a reserved word in Python\n",
    "    # Use 'from_' to pass the value to the function & here we'll change it back to 'from'.\n",
    "    if 'from_' in params:\n",
    "        params['from'] = params['from_']\n",
    "        del(params['from_'])\n",
    "    return params\n",
    "\n",
    "def get_cdx_data(params):\n",
    "    '''\n",
    "    Make a request to the CDX API using the supplied parameters.\n",
    "    Check the results for a resumption key, and return the key (if any) and the results.\n",
    "    '''\n",
    "    response = s.get('http://web.archive.org/cdx/search/cdx', params=params, headers={'User-Agent': ''})\n",
    "    response.raise_for_status()\n",
    "    results = response.json()\n",
    "    if not response.from_cache:\n",
    "        time.sleep(0.2)\n",
    "    return results\n",
    "\n",
    "def convert_lists_to_dicts(results):\n",
    "    if results:\n",
    "        keys = results[0]\n",
    "        results_as_dicts = [dict(zip(keys, v)) for v in results[1:]]\n",
    "    else:\n",
    "        results_as_dicts = results\n",
    "    return results_as_dicts\n",
    "\n",
    "def get_cdx_data_by_page(url, **kwargs):\n",
    "    page = 0\n",
    "    params = prepare_params(url, **kwargs)\n",
    "    total_pages = get_total_pages(params)\n",
    "    # We'll use a timestamp to distinguish between versions\n",
    "    timestamp = arrow.now().format('YYYYMMDDHHmmss')\n",
    "    file_path = Path(f'{slugify(domain)}-cdx-data-{timestamp}.ndjson')\n",
    "    # Remove any old versions of the data file\n",
    "    try:\n",
    "        file_path.unlink()\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    with tqdm(total=total_pages-page) as pbar1:\n",
    "        with tqdm() as pbar2:\n",
    "            while page < total_pages:\n",
    "                params['page'] = page\n",
    "                results = get_cdx_data(params)\n",
    "                with file_path.open('a') as f:\n",
    "                    writer = ndjson.writer(f, ensure_ascii=False)\n",
    "                    for result in convert_lists_to_dicts(results):\n",
    "                        writer.writerow(result)\n",
    "                page += 1\n",
    "                pbar1.update(1)\n",
    "                pbar2.update(len(results) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note than harvesting a domain has the same number of pages (ie requests) no matter what filters are applied -- it's just that some pages will be empty.\n",
    "# So repeating a domain harvest with different filters will mean less data, but the same number of requests.\n",
    "# What's most efficient? I dunno.\n",
    "get_cdx_data_by_page(f'*.{domain}', filter=['statuscode:200', 'mimetype:text/html'], collapse='urlkey', fl='urlkey,timestamp,original', pageSize=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the harvested data\n",
    "\n",
    "After many hours, and many interruptions, the harvesting process finally finished. I ended up with a 65gb ndjson file. How many captures does it include?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189,639,944\n",
      "CPU times: user 1min 5s, sys: 27.8 s, total: 1min 33s\n",
      "Wall time: 2min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count = 0\n",
    "with open('gov-au-cdx-data.ndjson') as f:\n",
    "    for line in f:\n",
    "        count += 1\n",
    "print(f'{count:,}')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find unique domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get extract a list of unique domains from all of those page captures. In the code below we extract domains from the `urlkey` and add them to a list. After every 100,000 lines, we use `set` to remove duplicates from the list. This is an attempt to find a reasonable balance between speed and memory consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4cb083961794ef29fd49153326b5ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 58s, sys: 38.7 s, total: 15min 37s\n",
      "Wall time: 15min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This is slow, but will avoid eating up memory\n",
    "domains = []\n",
    "with open('gov-au-cdx-data.ndjson') as f:\n",
    "    count = 0\n",
    "    with tqdm() as pbar:\n",
    "        for line in f:\n",
    "            capture = json.loads(line)\n",
    "            # Split the urlkey on ) to separate domain from path\n",
    "            domain = capture['urlkey'].split(')')[0]\n",
    "            # Remove port numbers\n",
    "            domain = re.sub(r'\\:\\d+', '', domain)\n",
    "            domains.append(domain)\n",
    "            count += 1\n",
    "            # Remove duplicates after every 100,000 lines to conserve memory\n",
    "            if count > 100000:\n",
    "                domains = list(set(domains))\n",
    "                pbar.update(count)\n",
    "                count = 0\n",
    "domains = list(set(domains))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique domains are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26233"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urlkey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>au,gov,consumersonline,maggie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>au,gov,nsw,leeton,libero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>au,gov,nsw,schools,burrenjunc-p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>au,gov,nsw,schools,bringelly-p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>au,gov,testcensus,stream0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            urlkey\n",
       "0    au,gov,consumersonline,maggie\n",
       "1         au,gov,nsw,leeton,libero\n",
       "2  au,gov,nsw,schools,burrenjunc-p\n",
       "3   au,gov,nsw,schools,bringelly-p\n",
       "4        au,gov,testcensus,stream0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(domains, columns=['urlkey'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the list of domains to a CSV file to save us having to extract them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='domains/gov-au-unique-domains.csv' target='_blank'>domains/gov-au-unique-domains.csv</a><br>"
      ],
      "text/plain": [
       "/Volumes/Workspace/mycode/glam-workbench/webarchives/notebooks/domains/gov-au-unique-domains.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.to_csv('domains/gov-au-unique-domains.csv', index=False)\n",
    "display(FileLink('domains/gov-au-unique-domains.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the list of domains from the CSV if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = pd.read_csv('domains/gov-au-unique-domains.csv')['urlkey'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of unique urls per subdomain\n",
    "\n",
    "Now that we have a list of unique domains we can use this to generate a count of unique urls per subdomain. This won't be exact. As noted previously, even with `collapse` set to `urlkey` there are likely to be duplicate urls. Getting rid of all the duplicates in such a large file would require a fair bit of processing, and I'm not sure it's worth it at this point. We really just want a sense of how subdomains are actually used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the domains as keys and the values set to zero\n",
    "domain_counts = dict(zip(domains, [0] * len(domains)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# FIND NUMBER OF URLS PER DOMAIN\n",
    "# As above we'll go though the file line by line\n",
    "# but this time we'll extract the domain and increment the corresponding value in the dict.\n",
    "with open('gov-au-cdx-data.ndjson') as f:\n",
    "    count = 0\n",
    "    with tqdm() as pbar:\n",
    "        for line in f:\n",
    "            capture = json.loads(line)\n",
    "            # Split the urlkey on ) to separate domain from path\n",
    "            domain = capture['urlkey'].split(')')[0]\n",
    "            domain = re.sub(r'\\:\\d+', '', domain)\n",
    "            # Increment domain count\n",
    "            domain_counts[domain] += 1\n",
    "            count += 1\n",
    "            # This is just to update the progress bar\n",
    "            if count > 100000:\n",
    "                pbar.update(count)\n",
    "                count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to a dataframe\n",
    "\n",
    "We'll now convert the data to a dataframe and do a bit more processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urlkey</th>\n",
       "      <th>number_of_pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>au,gov,consumersonline,maggie</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>au,gov,nsw,leeton,libero</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>au,gov,nsw,schools,burrenjunc-p</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>au,gov,nsw,schools,bringelly-p</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>au,gov,testcensus,stream0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            urlkey  number_of_pages\n",
       "0    au,gov,consumersonline,maggie              144\n",
       "1         au,gov,nsw,leeton,libero                1\n",
       "2  au,gov,nsw,schools,burrenjunc-p              187\n",
       "3   au,gov,nsw,schools,bringelly-p              197\n",
       "4        au,gov,testcensus,stream0                9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape dict as a list of dicts\n",
    "domain_counts_as_list = [{'urlkey': k, 'number_of_pages': v} for k, v in domain_counts.items()]\n",
    "\n",
    "# Convert to dataframe\n",
    "df_counts = pd.DataFrame(domain_counts_as_list)\n",
    "df_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to split the `urlkey` into its separate subdomains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the urlkey on commas into separate columns -- this creates a new df\n",
    "df_split = df_counts['urlkey'].str.split(',', expand=True)\n",
    "\n",
    "# Merge the new df back with the original so we have both the urlkey and it's components\n",
    "df_merged = pd.merge(df_counts, df_split, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll stich the subdomains back together in a traditional domain format just for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urlkey</th>\n",
       "      <th>number_of_pages</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>au,gov,consumersonline,maggie</td>\n",
       "      <td>144</td>\n",
       "      <td>au</td>\n",
       "      <td>gov</td>\n",
       "      <td>consumersonline</td>\n",
       "      <td>maggie</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>maggie.consumersonline.gov.au</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>au,gov,nsw,leeton,libero</td>\n",
       "      <td>1</td>\n",
       "      <td>au</td>\n",
       "      <td>gov</td>\n",
       "      <td>nsw</td>\n",
       "      <td>leeton</td>\n",
       "      <td>libero</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>libero.leeton.nsw.gov.au</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>au,gov,nsw,schools,burrenjunc-p</td>\n",
       "      <td>187</td>\n",
       "      <td>au</td>\n",
       "      <td>gov</td>\n",
       "      <td>nsw</td>\n",
       "      <td>schools</td>\n",
       "      <td>burrenjunc-p</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>burrenjunc-p.schools.nsw.gov.au</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>au,gov,nsw,schools,bringelly-p</td>\n",
       "      <td>197</td>\n",
       "      <td>au</td>\n",
       "      <td>gov</td>\n",
       "      <td>nsw</td>\n",
       "      <td>schools</td>\n",
       "      <td>bringelly-p</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>bringelly-p.schools.nsw.gov.au</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>au,gov,testcensus,stream0</td>\n",
       "      <td>9</td>\n",
       "      <td>au</td>\n",
       "      <td>gov</td>\n",
       "      <td>testcensus</td>\n",
       "      <td>stream0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>stream0.testcensus.gov.au</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            urlkey  number_of_pages   0    1                2  \\\n",
       "0    au,gov,consumersonline,maggie              144  au  gov  consumersonline   \n",
       "1         au,gov,nsw,leeton,libero                1  au  gov              nsw   \n",
       "2  au,gov,nsw,schools,burrenjunc-p              187  au  gov              nsw   \n",
       "3   au,gov,nsw,schools,bringelly-p              197  au  gov              nsw   \n",
       "4        au,gov,testcensus,stream0                9  au  gov       testcensus   \n",
       "\n",
       "         3             4     5     6     7     8     9  \\\n",
       "0   maggie          None  None  None  None  None  None   \n",
       "1   leeton        libero  None  None  None  None  None   \n",
       "2  schools  burrenjunc-p  None  None  None  None  None   \n",
       "3  schools   bringelly-p  None  None  None  None  None   \n",
       "4  stream0          None  None  None  None  None  None   \n",
       "\n",
       "                            domain  \n",
       "0    maggie.consumersonline.gov.au  \n",
       "1         libero.leeton.nsw.gov.au  \n",
       "2  burrenjunc-p.schools.nsw.gov.au  \n",
       "3   bringelly-p.schools.nsw.gov.au  \n",
       "4        stream0.testcensus.gov.au  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def join_domain(x):\n",
    "    parts = x.split(',')\n",
    "    parts.reverse()\n",
    "    return '.'.join(parts)\n",
    "\n",
    "df_merged['domain'] = df_merged['urlkey'].apply(join_domain)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='domains/gov-au-domains-split.csv' target='_blank'>domains/gov-au-domains-split.csv</a><br>"
      ],
      "text/plain": [
       "/Volumes/Workspace/mycode/glam-workbench/webarchives/notebooks/domains/gov-au-domains-split.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_merged.to_csv('domains/gov-au-domains-split.csv')\n",
    "display(FileLink('domains/gov-au-domains-split.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's count things!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many third level domains are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1720"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.unique(df_merged[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which third level domains have the most subdomains?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nsw         7477\n",
       "vic         3418\n",
       "qld         2772\n",
       "wa          2690\n",
       "sa          1719\n",
       "tas          957\n",
       "nt           752\n",
       "act          362\n",
       "embassy      151\n",
       "nla          138\n",
       "govspace     111\n",
       "deewr         77\n",
       "ga            75\n",
       "treasury      74\n",
       "ato           73\n",
       "health        73\n",
       "dest          69\n",
       "abs           61\n",
       "govcms        60\n",
       "bom           59\n",
       "Name: 2, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged[2].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which domains have the most unique pages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_1e008_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >domain</th>        <th class=\"col_heading level0 col1\" >number_of_pages</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_1e008_level0_row0\" class=\"row_heading level0 row0\" >3487</th>\n",
       "                        <td id=\"T_1e008_row0_col0\" class=\"data row0 col0\" >trove.nla.gov.au</td>\n",
       "                        <td id=\"T_1e008_row0_col1\" class=\"data row0 col1\" >9,285,603</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e008_level0_row1\" class=\"row_heading level0 row1\" >6159</th>\n",
       "                        <td id=\"T_1e008_row1_col0\" class=\"data row1 col0\" >nla.gov.au</td>\n",
       "                        <td id=\"T_1e008_row1_col1\" class=\"data row1 col1\" >2,592,182</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e008_level0_row2\" class=\"row_heading level0 row2\" >8705</th>\n",
       "                        <td id=\"T_1e008_row2_col0\" class=\"data row2 col0\" >collectionsearch.nma.gov.au</td>\n",
       "                        <td id=\"T_1e008_row2_col1\" class=\"data row2 col1\" >2,422,514</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e008_level0_row3\" class=\"row_heading level0 row3\" >8719</th>\n",
       "                        <td id=\"T_1e008_row3_col0\" class=\"data row3 col0\" >passwordreset.parliament.qld.gov.au</td>\n",
       "                        <td id=\"T_1e008_row3_col1\" class=\"data row3 col1\" >2,089,256</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e008_level0_row4\" class=\"row_heading level0 row4\" >22057</th>\n",
       "                        <td id=\"T_1e008_row4_col0\" class=\"data row4 col0\" >parlinfo.aph.gov.au</td>\n",
       "                        <td id=\"T_1e008_row4_col1\" class=\"data row4 col1\" >1,882,646</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e008_level0_row5\" class=\"row_heading level0 row5\" >23439</th>\n",
       "                        <td id=\"T_1e008_row5_col0\" class=\"data row5 col0\" >aph.gov.au</td>\n",
       "                        <td id=\"T_1e008_row5_col1\" class=\"data row5 col1\" >1,731,559</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e008_level0_row6\" class=\"row_heading level0 row6\" >12100</th>\n",
       "                        <td id=\"T_1e008_row6_col0\" class=\"data row6 col0\" >bmcc.nsw.gov.au</td>\n",
       "                        <td id=\"T_1e008_row6_col1\" class=\"data row6 col1\" >1,414,711</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e008_level0_row7\" class=\"row_heading level0 row7\" >4375</th>\n",
       "                        <td id=\"T_1e008_row7_col0\" class=\"data row7 col0\" >jobsearch.gov.au</td>\n",
       "                        <td id=\"T_1e008_row7_col1\" class=\"data row7 col1\" >1,293,760</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e008_level0_row8\" class=\"row_heading level0 row8\" >17414</th>\n",
       "                        <td id=\"T_1e008_row8_col0\" class=\"data row8 col0\" >arpansa.gov.au</td>\n",
       "                        <td id=\"T_1e008_row8_col1\" class=\"data row8 col1\" >1,278,603</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e008_level0_row9\" class=\"row_heading level0 row9\" >19663</th>\n",
       "                        <td id=\"T_1e008_row9_col0\" class=\"data row9 col0\" >abs.gov.au</td>\n",
       "                        <td id=\"T_1e008_row9_col1\" class=\"data row9 col1\" >961,526</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e008_level0_row10\" class=\"row_heading level0 row10\" >15325</th>\n",
       "                        <td id=\"T_1e008_row10_col0\" class=\"data row10 col0\" >libero.gtcc.nsw.gov.au</td>\n",
       "                        <td id=\"T_1e008_row10_col1\" class=\"data row10 col1\" >959,490</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e008_level0_row11\" class=\"row_heading level0 row11\" >15308</th>\n",
       "                        <td id=\"T_1e008_row11_col0\" class=\"data row11 col0\" >canterbury.nsw.gov.au</td>\n",
       "                        <td id=\"T_1e008_row11_col1\" class=\"data row11 col1\" >956,500</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e008_level0_row12\" class=\"row_heading level0 row12\" >12439</th>\n",
       "                        <td id=\"T_1e008_row12_col0\" class=\"data row12 col0\" >library.campbelltown.nsw.gov.au</td>\n",
       "                        <td id=\"T_1e008_row12_col1\" class=\"data row12 col1\" >932,933</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e008_level0_row13\" class=\"row_heading level0 row13\" >16309</th>\n",
       "                        <td id=\"T_1e008_row13_col0\" class=\"data row13 col0\" >defencejobs.gov.au</td>\n",
       "                        <td id=\"T_1e008_row13_col1\" class=\"data row13 col1\" >894,770</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e008_level0_row14\" class=\"row_heading level0 row14\" >5804</th>\n",
       "                        <td id=\"T_1e008_row14_col0\" class=\"data row14 col0\" >webopac.gosford.nsw.gov.au</td>\n",
       "                        <td id=\"T_1e008_row14_col1\" class=\"data row14 col1\" >854,395</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e008_level0_row15\" class=\"row_heading level0 row15\" >25031</th>\n",
       "                        <td id=\"T_1e008_row15_col0\" class=\"data row15 col0\" >library.lachlan.nsw.gov.au</td>\n",
       "                        <td id=\"T_1e008_row15_col1\" class=\"data row15 col1\" >838,972</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e008_level0_row16\" class=\"row_heading level0 row16\" >22341</th>\n",
       "                        <td id=\"T_1e008_row16_col0\" class=\"data row16 col0\" >library.shoalhaven.nsw.gov.au</td>\n",
       "                        <td id=\"T_1e008_row16_col1\" class=\"data row16 col1\" >800,541</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e008_level0_row17\" class=\"row_heading level0 row17\" >12284</th>\n",
       "                        <td id=\"T_1e008_row17_col0\" class=\"data row17 col0\" >catalogue.nla.gov.au</td>\n",
       "                        <td id=\"T_1e008_row17_col1\" class=\"data row17 col1\" >787,616</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e008_level0_row18\" class=\"row_heading level0 row18\" >5886</th>\n",
       "                        <td id=\"T_1e008_row18_col0\" class=\"data row18 col0\" >library.bankstown.nsw.gov.au</td>\n",
       "                        <td id=\"T_1e008_row18_col1\" class=\"data row18 col1\" >767,550</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1e008_level0_row19\" class=\"row_heading level0 row19\" >1963</th>\n",
       "                        <td id=\"T_1e008_row19_col0\" class=\"data row19 col0\" >myagedcare.gov.au</td>\n",
       "                        <td id=\"T_1e008_row19_col1\" class=\"data row19 col1\" >759,384</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x132c59940>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_20 = df_merged[['domain', 'number_of_pages']].sort_values(by='number_of_pages', ascending=False)[:20]\n",
    "top_20.style.format({'number_of_pages': '{:,}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there really domains made up of 10 levels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0-slwa.csiro.patron.eb20.com.henrietta.slwa.wa.gov.au',\n",
       " 'test-your-tired-self-prod.apps.p.dmp.aws.hosting.transport.nsw.gov.au',\n",
       " '0-www.library.eb.com.au.henrietta.slwa.wa.gov.au']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.loc[df_merged[9].notnull()]['domain'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualise things!\n",
    "\n",
    "I thought it would be interesting to try and visualise all the subdomains as a circular dendrogram. After a bit of investigation I discovered the [ETE Toolkit](http://etetoolkit.org/) for the visualisation of phylogenetic trees – it seemed perfect. But to get data into ETE I first had to convert it into a [Newick formatted](https://en.wikipedia.org/wiki/Newick_format) string. Fortunately, there's a [Python package](https://pypi.org/project/newick/) for that.\n",
    "\n",
    "Warning! While the code below will indeed generate circular dendrograms from a domain name hierarchy, if you have more than a few hundred domains you'll find that the image gets very big, very quickly. I successfully saved the whole of the `gov.au` domain as a 32mb SVG file, which you can (very slowly) view in a web browser or graphics program. But any attempt to save into another image format at a size that would make the text readable consumed huge amounts of memory and forced me to pull the plug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_domain_tree(domains):\n",
    "    '''\n",
    "    Converts a list of urlkeys into a Newick tree via nodes.\n",
    "    '''\n",
    "    d_tree = Node()\n",
    "    for domain in domains:\n",
    "        domain = re.sub(r'\\:\\d+', '', domain)\n",
    "        sds = domain.split(',')\n",
    "        for i, sd in enumerate(sds):\n",
    "            parent = '.'.join(reversed(sds[0:i])) if i > 0 else None\n",
    "            label = '.'.join(reversed(sds[:i+1]))\n",
    "            if not d_tree.get_node(label):\n",
    "                if parent:\n",
    "                    d_tree.get_node(parent).add_descendant(Node(label))\n",
    "                else:\n",
    "                    d_tree.add_descendant(Node(label))\n",
    "    return newick.dumps(d_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert domains to a Newick tree\n",
    "full_tree = make_domain_tree(domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dendrogram_to_file(tree, width, output_file):\n",
    "    t = Tree(tree, format=1)\n",
    "    circular_style = TreeStyle()\n",
    "    circular_style.mode = \"c\" # draw tree in circular mode\n",
    "    circular_style.optimal_scale_level = 'full'\n",
    "    circular_style.root_opening_factor = 0\n",
    "    circular_style.show_scale = False\n",
    "    t.render(output_file, w=width, tree_style=circular_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's play safe by creating a PNG with a fixed width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving a PNG with a fixed width will work, but you won't be able to read any text\n",
    "save_dendrogram_to_file(full_tree, 5000, 'images/govau-all-5000.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the result!\n",
    "\n",
    "![Circular dendrogram of all gov.au domains](images/govau-all-1000.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will save a zoomable SVG version that allows you to read the labels, but it will be very slow to use, and difficult to convert into other formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here be dendrodragons!\n",
    "# I don't think width does anything if you save to SVG\n",
    "save_dendrogram_to_file(full_tree, 5000, 'govau-all.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some third level domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dendrogram(label, level=2, df=df_merged, width=300):\n",
    "    domains = df.loc[df[2] == label]['urlkey'].to_list()\n",
    "    tree = make_domain_tree(domains)\n",
    "    save_dendrogram_to_file(tree, width, f'images/{label}-domains-{width}.png')\n",
    "    return f'<div style=\"width: 300px; float: left; margin-right: 10px;\"><img src=\"images/{label}-domains-{width}.png\" style=\"\"><p style=\"text-align: center;\">{label.upper()}</p></div>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width: 300px; float: left; margin-right: 10px;\"><img src=\"images/nsw-domains-300.png\" style=\"\"><p style=\"text-align: center;\">NSW</p></div><div style=\"width: 300px; float: left; margin-right: 10px;\"><img src=\"images/vic-domains-300.png\" style=\"\"><p style=\"text-align: center;\">VIC</p></div><div style=\"width: 300px; float: left; margin-right: 10px;\"><img src=\"images/qld-domains-300.png\" style=\"\"><p style=\"text-align: center;\">QLD</p></div><div style=\"width: 300px; float: left; margin-right: 10px;\"><img src=\"images/sa-domains-300.png\" style=\"\"><p style=\"text-align: center;\">SA</p></div><div style=\"width: 300px; float: left; margin-right: 10px;\"><img src=\"images/wa-domains-300.png\" style=\"\"><p style=\"text-align: center;\">WA</p></div><div style=\"width: 300px; float: left; margin-right: 10px;\"><img src=\"images/tas-domains-300.png\" style=\"\"><p style=\"text-align: center;\">TAS</p></div><div style=\"width: 300px; float: left; margin-right: 10px;\"><img src=\"images/nt-domains-300.png\" style=\"\"><p style=\"text-align: center;\">NT</p></div><div style=\"width: 300px; float: left; margin-right: 10px;\"><img src=\"images/act-domains-300.png\" style=\"\"><p style=\"text-align: center;\">ACT</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create dendrograms for each state/territory\n",
    "html = ''\n",
    "for state in ['nsw', 'vic', 'qld', 'sa', 'wa', 'tas', 'nt', 'act']:\n",
    "    html += display_dendrogram(state)\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are fewer domains you can see more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width: 300px; float: left; margin-right: 10px;\"><img src=\"images/act-domains-8000.png\" style=\"\"><p style=\"text-align: center;\">ACT</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "act = display_dendrogram(state, width=8000)\n",
    "display(HTML(act))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Created by [Tim Sherratt](https://timsherratt.org) for the [GLAM Workbench](https://glam-workbench.github.io).  Support me by becoming a [GitHub sponsor](https://github.com/sponsors/wragge)!\n",
    "\n",
    "Work on this notebook was supported by the [IIPC Discretionary Funding Programme 2019-2020](http://netpreserve.org/projects/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
